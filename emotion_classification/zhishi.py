import torch

x = torch.rand(3, 4)
print('x:\n', x)
print('torch.max(x,1):\n', torch.max(x, 1))
print('torch.max(x,0):\n', torch.max(x, 0))
print('torch.max(x,1)[0]:\n', torch.max(x, 1)[0])
print('torch.max(x,1)[1]:\n', torch.max(x, 1)[1])
print('torch.max(x,1)[1].data:\n', torch.max(x, 1)[1].data)
print('torch.max(x,1)[1].data.numpy():\n', torch.max(x, 1)[1].data.numpy())
print('torch.max(x,1)[1].data.numpy().squeeze():\n', torch.max(x, 1)[1].data.numpy().squeeze())
print('torch.max(x,1)[0].data:\n', torch.max(x, 1)[0].data)
print('torch.max(x,1)[0].data.numpy():\n', torch.max(x, 1)[0].data.numpy())
print('torch.max(x,1)[0].data.numpy().squeeze():\n', torch.max(x, 1)[0].data.numpy().squeeze())

outputs = torch.FloatTensor([[1], [2], [3]])
targets = torch.FloatTensor([[0], [2], [3]])
print('\n', torch.max(outputs.data, 1))
print(torch.max(outputs.data, 0))
print(targets.eq(outputs.data))
print(targets.eq(outputs.data).sum())

# Pytorch详解NLLLoss和CrossEntropyLoss
input = torch.randn(3, 3)
print('\n', input)
sm = torch.nn.Softmax(dim=1)
print('\n', sm(input))
logsm = torch.log(sm(input))
print('\n', logsm)
loss = torch.nn.NLLLoss()
target = torch.tensor([0, 2, 1])
print(loss(torch.log(sm(input)), target))
loss = torch.nn.CrossEntropyLoss()
print(loss(input, target))
